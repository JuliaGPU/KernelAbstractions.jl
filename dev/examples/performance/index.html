<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Measuring performance · KernelAbstractions.jl</title><link rel="canonical" href="https://juliagpu.github.io/KernelAbstractions.jl/examples/performance/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">KernelAbstractions.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../quickstart/">Quickstart</a></li><li><a class="tocitem" href="../../kernels/">Writing kernels</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../memcopy/">Memcopy</a></li><li><a class="tocitem" href="../memcopy_static/">Memcopy with static NDRange</a></li><li><a class="tocitem" href="../naive_transpose/">Naive Transpose</a></li><li class="is-active"><a class="tocitem" href>Measuring performance</a><ul class="internal"><li><a class="tocitem" href="#Results:-1"><span>Results:</span></a></li><li><a class="tocitem" href="#Code-1"><span>Code</span></a></li></ul></li><li><a class="tocitem" href="../matmul/">Matmul</a></li></ul></li><li><a class="tocitem" href="../../api/">API</a></li><li><span class="tocitem">Extras</span><ul><li><a class="tocitem" href="../../extras/unrolling/">Unroll macro</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Measuring performance</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Measuring performance</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/docs/src/examples/performance.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Measuring-performance-1"><a class="docs-heading-anchor" href="#Measuring-performance-1">Measuring performance</a><a class="docs-heading-anchor-permalink" href="#Measuring-performance-1" title="Permalink"></a></h1><p>Run under <code>nsight-cu</code>:</p><pre><code class="language-sh">nv-nsight-cu-cli --nvtx --profile-from-start=off --section=SpeedOfLight --section=julia --project=examples examples/performance.jl</code></pre><h2 id="Results:-1"><a class="docs-heading-anchor" href="#Results:-1">Results:</a><a class="docs-heading-anchor-permalink" href="#Results:-1" title="Permalink"></a></h2><p>Collated results on a V100:</p><table><tr><th style="text-align: right">Kernel</th><th style="text-align: right">Time</th><th style="text-align: right">Speed of Light Mem %</th></tr><tr><td style="text-align: right">naive (32, 32)</td><td style="text-align: right">1.19ms</td><td style="text-align: right">65.06%</td></tr><tr><td style="text-align: right">naive (1024, 1)</td><td style="text-align: right">1.79ms</td><td style="text-align: right">56.13 %</td></tr><tr><td style="text-align: right">naive (1, 1024)</td><td style="text-align: right">3.03ms</td><td style="text-align: right">60.02 %</td></tr></table><h3 id="Full-output:-1"><a class="docs-heading-anchor" href="#Full-output:-1">Full output:</a><a class="docs-heading-anchor-permalink" href="#Full-output:-1" title="Permalink"></a></h3><pre><code class="language-none">==PROF==   0: Naive transpose (32, 32)
    Section: GPU Speed Of Light
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Frequency                                                         cycle/usecond                         878.88
    SOL FB                                                                               %                          38.16
    Elapsed Cycles                                                                   cycle                      1,447,874
    SM Frequency                                                             cycle/nsecond                           1.23
    Memory [%]                                                                           %                          65.93
    Duration                                                                       msecond                           1.17
    SOL L2                                                                               %                          19.08
    SOL TEX                                                                              %                          66.19
    SM Active Cycles                                                                 cycle                   1,440,706.40
    SM [%]                                                                               %                          23.56
    ---------------------------------------------------------------------- --------------- ------------------------------

  ptxcall___gpu_transpose_kernel_naive__430_2, 2020-Feb-20 22:42:24, Context 1, Stream 23

==PROF==   0: Naive transpose (1024, 1)
    Section: GPU Speed Of Light
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Frequency                                                         cycle/usecond                         877.69
    SOL FB                                                                               %                          22.40
    Elapsed Cycles                                                                   cycle                      2,473,141
    SM Frequency                                                             cycle/nsecond                           1.23
    Memory [%]                                                                           %                          51.17
    Duration                                                                       msecond                           2.00
    SOL L2                                                                               %                          50.17
    SOL TEX                                                                              %                          51.27
    SM Active Cycles                                                                 cycle                   2,465,610.06
    SM [%]                                                                               %                          11.68
    ---------------------------------------------------------------------- --------------- ------------------------------

  ptxcall___gpu_transpose_kernel_naive__430_3, 2020-Feb-20 22:42:28, Context 1, Stream 25

==PROF==   0: Naive transpose (1, 1024)
    Section: GPU Speed Of Light
    ---------------------------------------------------------------------- --------------- ------------------------------
    Memory Frequency                                                         cycle/usecond                         876.69
    SOL FB                                                                               %                          17.88
    Elapsed Cycles                                                                   cycle                      3,737,127
    SM Frequency                                                             cycle/nsecond                           1.24
    Memory [%]                                                                           %                          60.02
    Duration                                                                       msecond                           3.02
    SOL L2                                                                               %                          60.02
    SOL TEX                                                                              %                          45.65
    SM Active Cycles                                                                 cycle                   3,732,591.59
    SM [%]                                                                               %                          12.56
    ---------------------------------------------------------------------- --------------- ------------------------------</code></pre><h2 id="Code-1"><a class="docs-heading-anchor" href="#Code-1">Code</a><a class="docs-heading-anchor-permalink" href="#Code-1" title="Permalink"></a></h2><pre><code class="language-julia">using KernelAbstractions, Test
include(joinpath(dirname(pathof(KernelAbstractions)), &quot;../examples/utils.jl&quot;)) # Load backend
using KernelAbstractions.Extras: @unroll

has_cuda &amp;&amp; has_cuda_gpu() || exit()
CUDA.allowscalar(false)

const nreps = 3
const N = 2048
const T = Float32

const TILE_DIM = 32
const BLOCK_ROWS = 8

# Simple variants

@kernel function simple_copy_kernel!(output, @Const(input))
  I, J = @index(Global, NTuple)
  @inbounds output[I, J] = input[I, J]
end

@kernel function simple_transpose_kernel!(output, @Const(input))
    I, J = @index(Global, NTuple)
    @inbounds output[I, J] = input[I, J]
end

# Local memory variants

@kernel function lmem_copy_kernel!(output, @Const(input), 
                                   ::Val{BANK}=Val(1)) where BANK
    I, J = @index(Global, NTuple)
    i, j = @index(Local,  NTuple)

    N = @uniform @groupsize()[1]
    M = @uniform @groupsize()[2]

    # +1 to avoid bank conflicts on shared memory
    tile = @localmem eltype(output) (N+BANK, M)

    @inbounds tile[i, j] = input[I, J]

    @synchronize

    @inbounds output[I, J] = tile[i, j]
end

@kernel function lmem_transpose_kernel!(output, @Const(input),
                                       ::Val{BANK}=Val(1)) where BANK
    gi, gj = @index(Group, NTuple)
    i, j = @index(Local,  NTuple)

    N = @uniform @groupsize()[1]
    M = @uniform @groupsize()[2]
    
    # +1 to avoid bank conflicts on shared memory
    tile = @localmem eltype(output) (N+BANK, M) 

    # Manually calculate global indexes
    # Later on we need to pivot the group index
    I = (gi-1) * N + i
    J = (gj-1) * M + j

    @inbounds tile[i, j] = input[I, J]

    @synchronize

    # Pivot the group index
    I = (gj-1) * M + i
    J = (gi-1) * N + j

    @inbounds output[I, J] = tile[j, i]
end

# Local Memory + process multiple elements per lane

@kernel function coalesced_copy_kernel!(output, @Const(input), 
                                        ::Val{BANK}=Val(1)) where BANK
    gi, gj = @index(Group, NTuple)
    i, j   = @index(Local, NTuple)

    TILE_DIM   = @uniform @groupsize()[1]
    BLOCK_ROWS = @uniform @groupsize()[2]

    # +1 to avoid bank conflicts on shared memory
    tile = @localmem eltype(output) (TILE_DIM+BANK, TILE_DIM)

    # Can&#39;t use @index(Global), because we use a smaller ndrange
    I = (gi-1) * TILE_DIM + i
    J = (gj-1) * TILE_DIM + j

    @unroll for k in 0:BLOCK_ROWS:(TILE_DIM-1)
        @inbounds tile[i, j+k] = input[I, J+k]
    end

    @synchronize

    @unroll for k in 0:BLOCK_ROWS:(TILE_DIM-1)
        @inbounds output[I, J+k] = tile[i, j+k]
    end
end

@kernel function coalesced_transpose_kernel!(output, @Const(input), 
                                             ::Val{BANK}=Val(1)) where BANK
    gi, gj = @index(Group, NTuple)
    i, j   = @index(Local, NTuple)

    TILE_DIM   = @uniform @groupsize()[1]
    BLOCK_ROWS = @uniform @groupsize()[2]

    # +1 to avoid bank conflicts on shared memory
    tile = @localmem eltype(output) (TILE_DIM+BANK, TILE_DIM)

    # Can&#39;t use @index(Global), because we use a smaller ndrange
    I = (gi-1) * TILE_DIM + i
    J = (gj-1) * TILE_DIM + j

    @unroll for k in 0:BLOCK_ROWS:(TILE_DIM-1)
        @inbounds tile[i, j+k] = input[I, J+k]
    end

    @synchronize

    # Transpose block offsets
    I = (gj-1) * TILE_DIM + i
    J = (gi-1) * TILE_DIM + j

    @unroll for k in 0:BLOCK_ROWS:(TILE_DIM-1)
        @inbounds output[I, J+k] = tile[j+k, i]
    end
end

# Benchmark simple

for block_dims in ((TILE_DIM, TILE_DIM), (TILE_DIM*TILE_DIM, 1), (1, TILE_DIM*TILE_DIM))
    for (name, kernel) in ( 
                            (&quot;copy&quot;,      simple_copy_kernel!(CUDADevice(), block_dims)),
                            (&quot;transpose&quot;, simple_transpose_kernel!(CUDADevice(), block_dims)),
                          )
        NVTX.@range &quot;Simple $name $block_dims&quot; let
            input = CUDA.rand(T, (N, N))
            output = similar(input)

            # compile kernel
            ev = kernel(input, output, ndrange=size(output))
            CUDA.@profile begin
                for rep in 1:nreps
                  ev = kernel(input, output, ndrange=size(output), dependencies=(ev,))
                end
                wait(ev)
            end
        end
    end
end

# Benchmark localmem
for (name, kernel) in ( 
                        (&quot;copy&quot;,      lmem_copy_kernel!(CUDADevice(), (TILE_DIM, TILE_DIM))),
                        (&quot;transpose&quot;, lmem_transpose_kernel!(CUDADevice(), (TILE_DIM, TILE_DIM))),
                      )
    for bank in (true, false)
        NVTX.@range &quot;Localmem $name ($TILE_DIM, $TILE_DIM) bank=$bank&quot; let
            input = CUDA.rand(T, (N, N))
            output = similar(input)

            # compile kernel
            ev = kernel(input, output, Val(Int(bank)), ndrange=size(output))
            CUDA.@profile begin
                for rep in 1:nreps
                    ev = kernel(input, output, Val(Int(bank)), ndrange=size(output), dependencies=(ev,))
                end
                wait(ev)
            end
        end
    end
end

# Benchmark localmem + multiple elements per lane
for (name, kernel) in ( 
                        (&quot;copy&quot;,      coalesced_copy_kernel!(CUDADevice(), (TILE_DIM, BLOCK_ROWS))),
                        (&quot;transpose&quot;, coalesced_transpose_kernel!(CUDADevice(), (TILE_DIM, BLOCK_ROWS))),
                      )
    for bank in (true, false)
        NVTX.@range &quot;Localmem + multiple elements $name ($TILE_DIM, $BLOCK_ROWS) bank=$bank&quot; let
            input = CUDA.rand(T, (N, N))
            output = similar(input)

            # We want a number of blocks equivalent to (TILE_DIM, TILE_DIM)
            # but our blocks are (TILE_DIM, BLOCK_ROWS) so we need to remove
            # a factor from the size of the array otherwise we get to many blocks
            block_factor = div(TILE_DIM, BLOCK_ROWS)
            ndrange = (N, div(N, block_factor))

            # compile kernel
            ev = kernel(input, output, Val(Int(bank)), ndrange=ndrange)
            CUDA.@profile begin
                for rep in 1:nreps
                    ev = kernel(input, output, Val(Int(bank)), ndrange=ndrange, dependencies=(ev,))
                end
                wait(ev)
            end
        end
    end
end
</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../naive_transpose/">« Naive Transpose</a><a class="docs-footer-nextpage" href="../matmul/">Matmul »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 19 November 2022 14:37">Saturday 19 November 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
